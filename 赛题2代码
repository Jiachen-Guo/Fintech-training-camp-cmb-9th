#ä¸‹é¢æœ‰ä¸€äº›ä¸éœ€è¦importï¼Œæœ‰ç‚¹æ‡’ä¸æƒ³åˆ äº†
#ä»£ç ä¾›å‚è€ƒï¼Œæœ‰éšæœºçš„å› ç´ ï¼Œå¹¶ä¸”å½“æ—¶ä¸€ç›´åœ¨è¯•ä¸åŒçš„æ¨¡å‹ï¼Œè¿™ä¸ªé…ç½®ä¸ä¸€å®šä¸æœ€ä¼˜é…ç½®å®Œå…¨ç›¸åŒ
import pandas as pd
import numpy as np
import torch
import copy
import torch.nn.functional as F
from transformers import AutoModel
from torch.nn.functional import softmax
from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
from sklearn.metrics import f1_score, accuracy_score
from sklearn.model_selection import train_test_split, StratifiedKFold
from torch import nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from transformers import (
    BertConfig, BertForMaskedLM, RobertaConfig, RobertaForMaskedLM, BertForSequenceClassification,
    Trainer, TrainingArguments, DataCollatorForLanguageModeling, BertModel
)
from torch.nn.utils.rnn import pad_sequence


train = pd.read_csv('train_text.csv')
test = pd.read_csv('test_text.csv')

train.drop(['æ–°é—»ID'],axis = 1, inplace = True)
test.drop(['æ–°é—»ID'],axis = 1, inplace = True)

#è®¾å®šPADï¼ŒMASKï¼ŒCLSå¯¹åº”çš„ID
PAD_ID = 0
MASK_ID = 1
CLS_ID = 2
#é€‰æ‹©offset = 101ä¸»è¦æ˜¯æ•°æ®é›†ä¸­ï¼Œæœ€å°çš„input_idsæ˜¯104ï¼Œä¸ºäº†ç•™å‡ºPADï¼ŒMASKï¼ŒCLSå¯¹åº”ä½ç½®
offset = 101
train['input_ids'] = train['æ–‡æœ¬'].apply(lambda x: [CLS_ID] + [int(i) - offset for i in x.split(' ')])
test['input_ids'] = test['æ–‡æœ¬'].apply(lambda x: [CLS_ID] + [int(i) - offset for i in x.split(' ')])

# trainã€teståˆå¹¶ç”¨äºæ— ç›‘ç£é¢„è®­ç»ƒ
# total_token_ids = train['input_ids']
total_token_ids = pd.concat([train['input_ids'], test['input_ids']], ignore_index=True)

# è®¡ç®— vocab size
vocab_size = max(max(seq) for seq in total_token_ids) + 1

train.rename(columns = {'æ ‡ç­¾':'label'}, inplace = True)
train['label'] = train['label']-1

#å®šä¹‰åˆ†ç±»ä»»åŠ¡æ•°æ®é›†
class ClassificationDataset(Dataset):
    def __init__(self, df, input_col='input_ids', label_col='label'):
        self.df = df.reset_index(drop=True)
        self.input_col = input_col
        self.label_col = label_col

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        input_ids = self.df.at[idx, self.input_col]
        label = self.df.at[idx, self.label_col]
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "label": torch.tensor(int(label), dtype=torch.long)
        }
#å®šä¹‰åˆ†ç±»ä»»åŠ¡æ•°æ®å¯¹é½å™¨ï¼Œæ¯æ‰¹input_idsæ˜¯å˜é•¿çš„ï¼Œå¯ä»¥åç»­è¯•è¯•å…¨éƒ¨å›ºå®šæˆmax_lengthçš„æ–¹å¼
class ClassificationCollator:
    def __init__(self, pad_token_id=PAD_ID, max_length=128):
        self.pad_token_id = pad_token_id
        self.max_length = max_length

    def __call__(self, batch):
        input_ids = [x["input_ids"][:self.max_length] for x in batch]
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)
        attention_mask = (input_ids != self.pad_token_id).long()
        label = torch.stack([x["label"] for x in batch])
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "label": label
        }

#å®šä¹‰MLMä»»åŠ¡æ•°æ®é›†
class CustomInputIDDataset(Dataset):
    def __init__(self, input_ids_list):
        self.input_ids_list = input_ids_list

    def __len__(self):
        return len(self.input_ids_list)

    def __getitem__(self, idx):
        return {"input_ids": torch.tensor(self.input_ids_list[idx], dtype=torch.long)}
        
#å®šä¹‰MLMä»»åŠ¡æ•°æ®å¯¹é½å™¨ï¼Œæ¯æ‰¹input_idsæ˜¯å˜é•¿çš„ï¼Œå¯ä»¥åç»­è¯•è¯•å…¨éƒ¨å›ºå®šæˆmax_lengthçš„æ–¹å¼
class CustomMLMCollator:
    def __init__(self, pad_token_id, mask_token_id, mlm_probability=0.15, max_length=128):
        self.pad_token_id = pad_token_id
        self.mask_token_id = mask_token_id
        self.mlm_probability = mlm_probability
        self.max_length = max_length

    def __call__(self, examples):
        input_ids = [e["input_ids"][:self.max_length] for e in examples]
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)

        label = input_ids.clone()
        rand = torch.rand(input_ids.shape)
        mask = (rand < self.mlm_probability) & (input_ids != self.pad_token_id)
        input_ids[mask] = self.mask_token_id
        label[~mask] = -100

        return {
            "input_ids": input_ids,
            "labels": label,
            "attention_mask": (input_ids != self.pad_token_id).long()
        }
#Robertaé…ç½®æ–‡ä»¶
config = RobertaConfig(
    vocab_size=vocab_size,
    hidden_size=512,
    num_attention_heads=8,
    num_hidden_layers=6,
    pad_token_id=PAD_ID,
    bos_token_id=CLS_ID,
    eos_token_id=PAD_ID,
    max_position_embeddings=64
)
#MLMä»»åŠ¡è®­ç»ƒ
roberta_model = RobertaForMaskedLM(config)
train_dataset = CustomInputIDDataset(total_token_ids.tolist())
collator = CustomMLMCollator(
    pad_token_id=PAD_ID,
    mask_token_id=MASK_ID,
    mlm_probability=0.2
)
training_args = TrainingArguments(
    output_dir="./mlm__roberta_model",
    num_train_epochs=50,  # å¤šè®­å‡ è½®
    per_device_train_batch_size=64,  # æ˜¾å­˜å…è®¸ï¼Œå¯ä»¥æé«˜
    gradient_accumulation_steps=1,  # å¦‚æœè¿˜ä¸å¤Ÿå¯ä»¥è®¾ç½®ä¸º2æˆ–4
    save_steps=500,
    save_total_limit=3,
    logging_steps=100,
    warmup_ratio=0.1,  # çƒ­èº«æœŸï¼Œé˜²æ­¢åˆæœŸå‘æ•£
    weight_decay=0.01,  # åŠ  L2 æ­£åˆ™
    learning_rate=3e-4,  # åˆç†å­¦ä¹ ç‡
    lr_scheduler_type='cosine_with_restarts',
    logging_dir="./logs",
    report_to="none"
)
trainer = Trainer(
    model=roberta_model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collator,
    tokenizer=None
)
trainer.train()

#æœ¬åœ°ä¿å­˜
pretrained_bert = trainer.model.roberta
pretrained_bert.save_pretrained('./mlm_Roberta_backup')


#[CLS] å®šä¹‰åˆ†ç±»æ¨¡å‹
class RobertaClassifier(nn.Module):
    def __init__(self, pretrained_roberta_encoder, num_labels, class_weights=None, dropout_prob=0.1):
        super().__init__()
        self.roberta = pretrained_roberta_encoder
        self.dropout = nn.Dropout(dropout_prob)
        # self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)
        self.hidden_size = self.roberta.config.hidden_size
        self.classifier = nn.Sequential(nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(), nn.Dropout(dropout_prob), nn.Linear(self.hidden_size, num_labels))
        # self.fc1 = nn.Linear(self.hidden_size, self.hidden_size)
        # self.relu = nn.ReLU()
        # self.norm = nn.LayerNorm(self.hidden_size)
        # self.out = nn.Linear(self.hidden_size, num_labels)
        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)
        # self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, label=None):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        
        # ç”¨ [CLS] token çš„è¾“å‡ºæ›¿ä»£ pooler_output
        pooled_output = outputs.last_hidden_state[:, 0]  # shape: [batch_size, hidden_dim]
        pooled_output = self.dropout(pooled_output)
        # residual = pooled_output
        # x = self.fc1(pooled_output)
        # x = self.relu(x)
        # x = self.norm(x + residual)
        # x = self.dropout(x)
        # logits = self.out(x)

        
        logits = self.classifier(pooled_output)

        loss = None
        if label is not None:
            loss = self.loss_fn(logits, label)

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}
#è®¾ç½®è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average='macro'),
        "f1_micro": f1_score(labels, preds, average='micro'),
        "f1_weighted": f1_score(labels, preds, average='weighted')
    }
#KFOLD
training_args_cls = TrainingArguments(
    output_dir="./cls_Roberta_model",
    num_train_epochs=10,
    per_device_train_batch_size=16,
    learning_rate=1e-4,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    greater_is_better=True,
    warmup_ratio=0.1,
    lr_scheduler_type='cosine_with_restarts',
    weight_decay=0.03,
    logging_dir="./logs_cls_Roberta",
    logging_steps=50,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    report_to="none",
    max_grad_norm=1.0
)
test_df = test[['input_ids']]
test_df['label'] = 1
test_dataset = ClassificationDataset(test_df, input_col='input_ids', label_col='label')
num_class = train['label'].nunique()
oof_preds = np.zeros(len(train))  # è®­ç»ƒé›† OOF é¢„æµ‹ç±»åˆ«
oof_preds_roberta = np.zeros((len(train), num_class)) # è®­ç»ƒé›† OOF é¢„æµ‹æ¦‚ç‡
test_preds = np.zeros((len(test), num_class))    # æµ‹è¯•é›†é¢„æµ‹æ¦‚ç‡
df = train[['input_ids', 'label']].reset_index(drop=True)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):
    print(f"\nğŸŸ¦ Fold {fold + 1}")
    train_split = df.iloc[train_idx].reset_index(drop=True)
    valid_split = df.iloc[val_idx].reset_index(drop=True)
    #è®¡ç®—ä¸åŒç±»åˆ«æƒé‡
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_split['label']), y=train_split['label'])
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)
    roberta = AutoModel.from_pretrained('./mlm_Roberta_backup')
    #ç”Ÿæˆç»§æ‰¿MLMä»»åŠ¡è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹
    model = RobertaClassifier(roberta, num_labels=num_class, class_weights=class_weights_tensor, dropout_prob = 0.1)
    # model = RobertaClassifier(roberta, num_labels=num_class, class_weights=class_weights_tensor, use_supcon=True, supcon_weight=0.2)
    cls_train_dataset = ClassificationDataset(train_split, input_col='input_ids', label_col='label')
    cls_val_dataset = ClassificationDataset(valid_split, input_col='input_ids', label_col='label')

    trainer = Trainer(
        model=model,
        args=training_args_cls,
        train_dataset=cls_train_dataset,
        eval_dataset=cls_val_dataset,
        tokenizer=None,
        data_collator=ClassificationCollator(pad_token_id=PAD_ID),
        compute_metrics=compute_metrics
    )
    #è®­ç»ƒ
    trainer.train()
    fold_val_probs = trainer.predict(cls_val_dataset).predictions
    fold_test_probs = trainer.predict(test_dataset).predictions
    oof_preds_roberta[val_idx,:] = fold_val_probs
    pred_labels = fold_val_probs.argmax(axis=-1)
    oof_preds[val_idx] = pred_labels
    test_preds += fold_test_probs
